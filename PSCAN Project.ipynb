{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhzk3GE6S9RC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d57c23-384a-4082-91f2-bf0e7e6c3b80"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install graphframes"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.25.2)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-3.3.2-bin-hadoop2\")# SPARK_HOME\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from graphframes import GraphFrame\n",
        "import math\n",
        "from collections import *\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "LNmIwYZL_GPT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/sparkling-graph/sparkling-graph/raw/master/loaders/src/test/resources/simple.csv -O simple.csv\n",
        "simple_data = spark.read.csv(\"simple.csv\", header = True, inferSchema= True)\n",
        "simple_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUP1ognR_w0f",
        "outputId": "20686deb-99c8-48d1-bb0a-43b1e3063ec1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-08 04:28:51--  https://github.com/sparkling-graph/sparkling-graph/raw/master/loaders/src/test/resources/simple.csv\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sparkling-graph/sparkling-graph/master/loaders/src/test/resources/simple.csv [following]\n",
            "--2024-04-08 04:28:51--  https://raw.githubusercontent.com/sparkling-graph/sparkling-graph/master/loaders/src/test/resources/simple.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49 [text/plain]\n",
            "Saving to: ‘simple.csv’\n",
            "\n",
            "simple.csv          100%[===================>]      49  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-08 04:28:52 (2.35 MB/s) - ‘simple.csv’ saved [49/49]\n",
            "\n",
            "root\n",
            " |-- v1: integer (nullable = true)\n",
            " |-- v2: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"simple.csv\"\n",
        "edge_adjacency_list_all = defaultdict(list)\n",
        "\n",
        "def csv_to_adjacency_list(file_path):\n",
        "    # Read the CSV file\n",
        "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "    # Group by src and aggregate dst into a list\n",
        "    adjacency_list_df = df.groupBy(\"v1\").agg(collect_list(\"v2\").alias(\"adjacency_list\"))\n",
        "\n",
        "    # Handle the undirected nature of the graph\n",
        "    df_swapped = df.selectExpr(\"v2 as v1\", \"v1 as v2\")\n",
        "    df_undirected = df.union(df_swapped)\n",
        "\n",
        "    # Group by vertex and aggregate its neighbors to form the adjacency list\n",
        "    adjacency_list_df = df_undirected.groupBy(\"v1\").agg(collect_list(\"v2\").alias(\"v_adjacency_list\")).sort(asc(\"v1\"))\n",
        "\n",
        "    # Output the adjacency list for all vertices\n",
        "    adjacency_list_df.show()\n",
        "\n",
        "    adjacency_list =  {row['v1']: row['v_adjacency_list'] for row in adjacency_list_df.collect()}\n",
        "\n",
        "    return adjacency_list\n",
        "\n",
        "'''\n",
        "PCSS Mapper:\n",
        "Input: key is the input vertex, value is the adjacency list of\n",
        "the input vertex\n",
        "Output: < key, value >, where the key is the edge\n",
        "connecting the input vertex and its neighbor and value is\n",
        "the adjacency list of the input vertex\n",
        "'''\n",
        "def PCSS_Mapper(v, v_adjacency_list):\n",
        "    edge_adjacency_pair = {}\n",
        "\n",
        "    # For each neighbor vi in the adjacency list\n",
        "    for vi in v_adjacency_list:\n",
        "        if v < vi:\n",
        "            # If v < vi, Take (v, vi) as key\n",
        "            key = (v, vi)\n",
        "        else:\n",
        "            # else, Take (vi, v) as key\n",
        "            key = (vi, v)\n",
        "        # Take the adjacency list of vertex v as value\n",
        "        value = v_adjacency_list\n",
        "        edge_adjacency_pair[key] = value\n",
        "\n",
        "    # Return {((v,vi), v_adjecency_list), (v,vj), v_adjecency_list)...}\n",
        "    return edge_adjacency_pair\n",
        "\n",
        "'''\n",
        "PCSS_Combiner\n",
        "Input: key is edge, value is the adjacency list of one vertex of the edge\n",
        "Output: < key, value >, where key is edge, values is the adjacency list of both vertexes of the edge\n",
        "'''\n",
        "def PCSS_Combiner(dicts_list):\n",
        "    for d in dicts_list:\n",
        "        for k, v in d.items():\n",
        "            if v not in edge_adjacency_list_all[k]:\n",
        "                # Append a new list containing v for each dictionary\n",
        "                edge_adjacency_list_all[k].append(v)\n",
        "    return dict(edge_adjacency_list_all)\n",
        "\n",
        "'''\n",
        "PCSS_Reducer and Cutting_Edge\n",
        "Input: key is one edge, values includes the adjacency lists\n",
        "of the two vertices adjacent to the edge\n",
        "Output: < key, value >, where the key is the edge and\n",
        "value is structural similarity of the edge\n",
        "'''\n",
        "def PCSS_Reducer_Cutting_Edge(edge_neighbors_dict, epsilon):\n",
        "    similarity_dict = defaultdict(list)\n",
        "    for k,v in edge_neighbors_dict.items():\n",
        "        #calculate structural similarity\n",
        "        similarity = len(list(set(v[0])&set(v[1])))/math.sqrt(len(v[0])*len(v[1]))\n",
        "\n",
        "        #prune the edges with only similarity is bigger than threshold\n",
        "        if similarity >= epsilon:\n",
        "            similarity_dict[k].append(similarity)\n",
        "    return dict(similarity_dict)\n",
        "\n",
        "'''\n",
        "pre_LPCC_parser\n",
        "Input: key is one edge, value is structural similarity of the edge\n",
        "Output: < key, value >, where the key is the vertex and\n",
        "value is the adjacency list of the input vertex\n",
        "'''\n",
        "def pre_LPCC_parser(reduced_simi_dict):\n",
        "    reduced_edges = []\n",
        "    for k,v in reduced_simi_dict.items():\n",
        "        reduced_edges.append(k)\n",
        "\n",
        "    # Parallelize the list to create an RDD\n",
        "    edges_rdd = sc.parallelize(reduced_edges)\n",
        "\n",
        "    # Generate (vertex, adjacent) pairs for both directions\n",
        "    adjacency_pairs = edges_rdd.flatMap(lambda edge: [(edge[0], edge[1]), (edge[1], edge[0])])\n",
        "\n",
        "    # Group by key (vertex) and map values to form the adjacency list\n",
        "    adjacency_list = adjacency_pairs.groupByKey().mapValues(list)\n",
        "\n",
        "    # return value as adjacency list in dict: {1: [2,3], 2:[1,3],3:[1,2]}\n",
        "    result = adjacency_list.collectAsMap()\n",
        "    sorted_result = {k: result[k] for k in sorted(result)}\n",
        "    return sorted_result\n",
        "\n",
        "'''\n",
        "LPCC\n",
        "Input: A network in the format of adjacency list\n",
        "Output: All of the connected components in the network\n",
        "'''\n",
        "def LPCC(reduced_adj_list):\n",
        "    # initialize the input to be (vertex ID, structure information),\n",
        "    # in which the structure information includes status, label and adjacency list of the vertex\n",
        "    stuct_info_v = {k: (1, k, v) for k, v in reduced_adj_list.items()}\n",
        "    print(stuct_info_v)\n",
        "    for k,v in stuct_info_v.items():\n",
        "        kv = LPCC_Mapper(k,v)\n",
        "        LPCC_Reducer(kv)\n",
        "\n",
        "'''\n",
        "LPCC_Mapper\n",
        "Input: key is the input vertex ID, value is the structure\n",
        "information (status, label and adjacency list) of the input\n",
        "vertex\n",
        "Output: < key, value >, where the key is a vertex ID\n",
        "and value is the label or the structure information of the\n",
        "input vertex\n",
        "'''\n",
        "def LPCC_Mapper(v,struct_info):\n",
        "    status = struct_info[0]\n",
        "    label = struct_info[1]\n",
        "    adj_list = struct_info[2]\n",
        "\n",
        "    #update v and its neighbors\n",
        "    emit_list = {v:[]}\n",
        "    # if v is activated, update its neighors:\n",
        "    if status == 1:\n",
        "        for v_i in adj_list:\n",
        "            emit_list[v].append((v_i, label))\n",
        "    # the structure information of the input vertex\n",
        "    emit_list[v].append((struct_info))\n",
        "    print('EMIT-list', emit_list)\n",
        "    return emit_list\n",
        "\n",
        "'''\n",
        "LPCC_Reducer\n",
        "Input: key is vertex ID, values includes the structure\n",
        "information of the vertex and the labels from its neighbors.\n",
        "Output: < key, value >, where the key is the vertex ID\n",
        "and value is the updated structure information of the vertex\n",
        "'''\n",
        "def LPCC_Reducer(emit_list_from_mapper):\n",
        "    # extract k-v pair from input, we know there is only one v\n",
        "    v = next(iter(emit_list_from_mapper.keys()))\n",
        "    values = next(iter(emit_list_from_mapper.values()))\n",
        "    struct_info = list(values[-1])\n",
        "    # Find the current label\n",
        "    current_label = struct_info[1]\n",
        "\n",
        "    # Find the smallest label from the neighbors\n",
        "    neighbors = list(values[:-1])\n",
        "    smallest_item = neighbors[0]\n",
        "    for item in neighbors:\n",
        "        if item[1] < smallest_item[1]:\n",
        "            smallest_item = item\n",
        "\n",
        "    # If the smallest label from its neighbors is less than its current label\n",
        "    if smallest_item[1] < current_label:\n",
        "        # Set the vertex as activated\n",
        "        struct_info[0] = 1\n",
        "        # update the label in the structure information as the smallest label\n",
        "        struct_info[1] = smallest_item[1]\n",
        "    else:\n",
        "        struct_info[0] = 0\n",
        "\n",
        "    updated_struct_info = {v: struct_info}\n",
        "    print(updated_struct_info)\n",
        "    return updated_struct_info"
      ],
      "metadata": {
        "id": "Dg41w6w8Ey52"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_dict = []\n",
        "adjacency_list_all = csv_to_adjacency_list(file_path)\n",
        "print(adjacency_list_all)\n",
        "\n",
        "print(edge_adjacency_list_all)\n",
        "\n",
        "for  key, value in adjacency_list_all.items():\n",
        "    e_adjacency_list = PCSS_Mapper(key, value)\n",
        "    print(e_adjacency_list)\n",
        "    temp_dict.append(e_adjacency_list)\n",
        "\n",
        "print('Temp_dict', temp_dict)\n",
        "\n",
        "en_dict = PCSS_Combiner(temp_dict)\n",
        "print('PCSS_dict', en_dict)\n",
        "\n",
        "reduced_graph = PCSS_Reducer_Cutting_Edge(en_dict,0.2)\n",
        "print('Reduced Graph', reduced_graph)\n",
        "\n",
        "reduced_adj_list = pre_LPCC_parser(reduced_graph)\n",
        "print(reduced_adj_list)\n",
        "\n",
        "LPCC(reduced_adj_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c89-ez6DKEvt",
        "outputId": "7bb7e707-1625-48ef-ab33-b229c7318c43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+\n",
            "| v1|v_adjacency_list|\n",
            "+---+----------------+\n",
            "|  1|       [2, 4, 3]|\n",
            "|  2|          [3, 1]|\n",
            "|  3|          [1, 2]|\n",
            "|  4|             [1]|\n",
            "|  5|             [6]|\n",
            "|  6|             [5]|\n",
            "+---+----------------+\n",
            "\n",
            "{1: [2, 4, 3], 2: [3, 1], 3: [1, 2], 4: [1], 5: [6], 6: [5]}\n",
            "defaultdict(<class 'list'>, {(1, 2): [[2, 4, 3], [3, 1]], (1, 4): [[2, 4, 3], [1]], (1, 3): [[2, 4, 3], [1, 2]], (2, 3): [[3, 1], [1, 2]], (5, 6): [[6], [5]]})\n",
            "{(1, 2): [2, 4, 3], (1, 4): [2, 4, 3], (1, 3): [2, 4, 3]}\n",
            "{(2, 3): [3, 1], (1, 2): [3, 1]}\n",
            "{(1, 3): [1, 2], (2, 3): [1, 2]}\n",
            "{(1, 4): [1]}\n",
            "{(5, 6): [6]}\n",
            "{(5, 6): [5]}\n",
            "Temp_dict [{(1, 2): [2, 4, 3], (1, 4): [2, 4, 3], (1, 3): [2, 4, 3]}, {(2, 3): [3, 1], (1, 2): [3, 1]}, {(1, 3): [1, 2], (2, 3): [1, 2]}, {(1, 4): [1]}, {(5, 6): [6]}, {(5, 6): [5]}]\n",
            "PCSS_dict {(1, 2): [[2, 4, 3], [3, 1]], (1, 4): [[2, 4, 3], [1]], (1, 3): [[2, 4, 3], [1, 2]], (2, 3): [[3, 1], [1, 2]], (5, 6): [[6], [5]]}\n",
            "Reduced Graph {(1, 2): [0.4082482904638631], (1, 3): [0.4082482904638631], (2, 3): [0.5]}\n",
            "{1: [2, 3], 2: [1, 3], 3: [1, 2]}\n",
            "{1: (1, 1, [2, 3]), 2: (1, 2, [1, 3]), 3: (1, 3, [1, 2])}\n",
            "EMIT-list {1: [(2, 1), (3, 1), (1, 1, [2, 3])]}\n",
            "{1: [0, 1, [2, 3]]}\n",
            "EMIT-list {2: [(1, 2), (3, 2), (1, 2, [1, 3])]}\n",
            "{2: [0, 2, [1, 3]]}\n",
            "EMIT-list {3: [(1, 3), (2, 3), (1, 3, [1, 2])]}\n",
            "{3: [0, 3, [1, 2]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PSCAN: orignal code in SCALA, partial implementation from the authors\n",
        "https://github.com/dawnranger/spark-pscan/blob/master/README.md\n",
        "\n",
        "@inproceedings{zhaorepo,\n",
        "  author = {Zhao, Weizhong},\n",
        "  title = {https://github.com/dawnranger/spark-pscan},\n",
        "  year = {2018},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\url{https://github.com/dawnranger/spark-pscan}},\n",
        "  commit = {4e1cc87add0feae6c9027610d6ff789787234852}\n",
        "}\n",
        "\n",
        "'''\n",
        "\n",
        "from graphframes import GraphFrame\n",
        "from pyspark.sql.functions import col, explode, array, size, sqrt\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def pscan(graph, epsilon=0.5):\n",
        "    # Assume `graph` is a GraphFrame object\n",
        "\n",
        "    # Extracting vertices and edges from the graph\n",
        "    vertices = graph.vertices\n",
        "    edges = graph.edges\n",
        "\n",
        "    # Creating a DataFrame of neighbors\n",
        "    edges_both_directions = edges.select(col(\"src\").alias(\"id\"), col(\"dst\").alias(\"neighbor\"))\\\n",
        "        .unionByName(edges.select(col(\"dst\").alias(\"id\"), col(\"src\").alias(\"neighbor\")))\n",
        "\n",
        "    # Grouping neighbors together by vertex\n",
        "    neighbors = edges_both_directions.groupBy(\"id\").agg(F.collect_set(\"neighbor\").alias(\"neighbors\"))\n",
        "\n",
        "    # Adding self to neighbor set\n",
        "    vertices_with_neighbors = vertices.join(neighbors, \"id\", \"left_outer\")\\\n",
        "        .withColumn(\"neighbors\", F.array_union(col(\"neighbors\"), array(col(\"id\"))))\\\n",
        "        .na.fill({\"neighbors\": []})\n",
        "\n",
        "    # Joining neighbors information to edges for similarity calculation\n",
        "    edges_with_neighbors = edges.join(vertices_with_neighbors.withColumnRenamed(\"id\", \"src\").withColumnRenamed(\"neighbors\", \"src_neighbors\"), \"src\")\\\n",
        "        .join(vertices_with_neighbors.withColumnRenamed(\"id\", \"dst\").withColumnRenamed(\"neighbors\", \"dst_neighbors\"), \"dst\")\n",
        "\n",
        "    # Calculating similarity\n",
        "    def calculate_similarity(src_neighbors, dst_neighbors):\n",
        "        intersection = size(array_intersect(col(src_neighbors), col(dst_neighbors)))\n",
        "        denominator = sqrt(size(col(src_neighbors)) * size(col(dst_neighbors)))\n",
        "        return intersection / denominator\n",
        "\n",
        "    edges_with_similarity = edges_with_neighbors.withColumn(\"similarity\", calculate_similarity(\"src_neighbors\", \"dst_neighbors\"))\n",
        "\n",
        "    # Filtering edges based on similarity\n",
        "    edges_filtered = edges_with_similarity.filter(col(\"similarity\") >= epsilon)\n",
        "\n",
        "    # Constructing a new graph with filtered edges\n",
        "    graph_filtered = GraphFrame(vertices, edges_filtered.select(col(\"src\"), col(\"dst\")))\n",
        "\n",
        "    # Finding connected components (clusters)\n",
        "    components = graph_filtered.connectedComponents()\n",
        "\n",
        "    # Joining the cluster IDs back to the original vertices\n",
        "    vertices_with_components = vertices.join(components, \"id\")\n",
        "\n",
        "    # Returning a new graph with vertices labeled by their component ID\n",
        "    return GraphFrame(vertices_with_components, edges)\n",
        "\n"
      ],
      "metadata": {
        "id": "5mvOIVn7e0lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byHkCyaEGMkc",
        "outputId": "be0fd04a-2103-463f-d417-b507b7c5e083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n"
          ]
        }
      ]
    }
  ]
}
